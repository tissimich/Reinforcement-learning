{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQcDGnzOHkou"
      },
      "source": [
        "# Solving Frozen Lake Problem Using Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0YBBoWNHko7"
      },
      "source": [
        "Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over the Frozen(F) lake.  \n",
        "The agent may not always move in the intended direction due to the slippery nature of the frozen lake. It will move in the intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions.  \n",
        "For example, if action is left, then:  \n",
        "        - P(move left)=1/3  \n",
        "        - P(move up)=1/3  \n",
        "        - P(move down)=1/3  \n",
        "\n",
        "\n",
        "### Action Space  \n",
        "\n",
        "The agent takes a 1-element vector for actions. The action space is `(dir)`, where `dir` decides direction to move in which can be:  \n",
        "    - 0: LEFT  \n",
        "    - 1: DOWN  \n",
        "    - 2: RIGHT  \n",
        "    - 3: UP  \n",
        "    \n",
        " ### Observation Space  \n",
        " \n",
        "The observation is a value representing the agent's current position as current_row * nrows + current_col (where both the row and col start at 0). For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
        "The number of possible observations is dependent on the size of the map.  \n",
        "For example, the 4x4 map has 16 possible observations.  \n",
        "\n",
        "\n",
        "### Rewards  \n",
        "  \n",
        "Reward schedule:  \n",
        "    - Reach goal(G): +1  \n",
        "    - Reach hole(H): 0  \n",
        "    - Reach frozen(F): 0  \n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJSAOCMHko_"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWG8KtXGHkpC"
      },
      "source": [
        "Initialize the Frozen Lake environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5seJg11hHkpD"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr6s2lKcHkpE"
      },
      "source": [
        " Let's take a look at how the environment looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8xKTj3BHkpF",
        "outputId": "a416942e-25b3-405b-86cc-bc5149bda404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex3kXhNJHkpO"
      },
      "source": [
        "Our agent is in the state S and the goal is to reach state G without visiting the H states.\n",
        "\n",
        "Now we define the function called value_iteration which performs value iteration and returns the optimal value\n",
        "function (value table)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBGdX_MkHkpQ"
      },
      "outputs": [],
      "source": [
        "gamma=1.0\n",
        "\n",
        "def value_iteration(env):\n",
        "    \n",
        "    # initialize value table with zeros\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "    \n",
        "    # set number of iterations and threshold\n",
        "    num_iterations = 100000\n",
        "    threshold = 1e-22\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        # On each iteration, copy the value table to the updated_value_table\n",
        "        updated_value_table = np.copy(value_table) \n",
        "        \n",
        "        # Now we calculate Q Value for each actions in the state \n",
        "        # and update the value of a state with maximum Q value        \n",
        "        for state in range(env.observation_space.n):\n",
        "            Q_values = [sum([trans_prob*(reward + gamma*updated_value_table[next_state])\n",
        "                             for trans_prob, next_state, reward, _ in env.P[state][action]]) \n",
        "                                   for action in range(env.action_space.n)] \n",
        "                                                        \n",
        "            value_table[state] = max(Q_values)            \n",
        "        # we check whether we have reached the convergence, that is, whether the difference \n",
        "        # between our value table and updated value table is less than a threshold value. \n",
        "        # If it is, we break the loop and return the value function as optimal value function\n",
        "        \n",
        "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "             print ('Value-iteration converged at iteration #', i+1)\n",
        "             break\n",
        "    \n",
        "    return value_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhlMvm3GHkpT"
      },
      "source": [
        "Now, we define a function called extract_policy for extracting optimal policy from the optimal value function.  \n",
        "We calculate Q value using our optimal value function and pick up\n",
        "the actions which has the highest Q value for each state as the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6-GrfKGHkpU"
      },
      "outputs": [],
      "source": [
        "def extract_policy(value_table):\n",
        " \n",
        "    # initialize the policy with zeros\n",
        "    policy = np.zeros(env.observation_space.n)     \n",
        "    \n",
        "    for state in range(env.observation_space.n):\n",
        "                \n",
        "        # compute Q value for all ations in the state\n",
        "        for action in range(env.action_space.n):\n",
        "            Q_values = [sum([trans_prob*(reward + gamma*value_table[next_state])\n",
        "                             for trans_prob, next_state, reward, _ in env.P[state][action]]) \n",
        "                                   for action in range(env.action_space.n)] \n",
        "                \n",
        "        #extract policy by selecting the action which has maximum Q value\n",
        "        policy[state] = np.argmax(np.array(Q_values))        \n",
        "    \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol4l0ipJHkpV"
      },
      "source": [
        "First, We compute the optimal value function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXhHyS81HkpW",
        "outputId": "8c7143a0-772f-4147-db04-cae483a77968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value-iteration converged at iteration # 1373\n"
          ]
        }
      ],
      "source": [
        "optimal_value_function = value_iteration(env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTjjD2tAHkpX"
      },
      "source": [
        "and we derive the optimal policy from the optimal value function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT0bVVkJHkpY"
      },
      "outputs": [],
      "source": [
        "optimal_policy = extract_policy(optimal_value_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-pX8ZoTHkpZ",
        "outputId": "e006af78-d581-484d-ef2d-ee8e0c271184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(optimal_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMCGl0TAHkpa",
        "outputId": "a919cd9d-ae0c-472a-9811-2569f56c409f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMJLo6fTHkpb"
      },
      "source": [
        "Best policy for each state:  \n",
        "\n",
        "[0. 3. 3. 3.   \n",
        " 0. 0. 0. 0.   \n",
        " 3. 1. 0. 0.   \n",
        " 0. 2. 1. 0.]  \n",
        "\n",
        "If you are in state 1, for example, the best policy is 3 (go up). This will make it move in the intended direction with probability of 1/3, else it will move in any perpendicular direction (left or right) with equal probability of 1/3 in both directions. Which means that with this policy the agent will not fall into the hole below.\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "name": "Solving Frozen Lake Problem  using Value Iteration .ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}